from typing import Optional
from matplotlib import pyplot as plt
from pathlib import Path
import torch
import pandas as pd
import numpy as np
from src.data import HarDataset, DataModule, MediaDir
from src.data.util import OutDir
from src.eval import ClipPlot, PlotIterator, EvaluationModule, MultiLabelStatScores, MultiLabelStatCurves


class ReportEvaluationModule(EvaluationModule):

    def __init__(self, out_dir: str, data_module: DataModule, report: pd.DataFrame, logger, img_format='eps',
                 consensus='max'):
        super().__init__(out_dir, data_module, logger, img_format)

        assert consensus in ['max', 'avg']

        self.report = report
        self.consensus = consensus
        self.train_scalars: [MultiLabelStatScores] = [None for _ in range(self.num_epochs)]
        self.val_scalars: [MultiLabelStatScores] = [None for _ in range(self.num_epochs)]
        self.test_scalars: [MultiLabelStatScores] = [None for _ in range(100)]

        self.train_curve: Optional[MultiLabelStatCurves] = None
        self.val_curve: Optional[MultiLabelStatCurves] = None
        self.test_curve: Optional[MultiLabelStatCurves] = None

    def get_sample_plot_by_report(self, context='train', label=None, epoch=None):
        if epoch is None:
            epoch = self.num_epochs - 1

        if context == 'test':
            df = self.test_df
            epoch = self.last_test_epoch
        else:
            df = self.report

        df = df[(df.subset == context) & (df.epoch == epoch)]

        if label is not None:
            df = df[df.labels.str.contains(label, na=False)]

        sample = df.sample()

        print(f'{sample.key.item()}@{sample.start.item()} - {sample.labels.item()}')

        # todo: context=all
        print(context)
        dataset_idx = self.dm.datasets[context].get_row(sample.key.item(), sample.start.item())

        preds = np.fromstring(sample.scores.item()[1:-1], dtype=float, sep=', ')
        preds = torch.from_numpy(preds)
        return self.get_sample_plot(row=dataset_idx, context=context, pred=preds)

    def get_top_loss_plots(self, context='train', epoch=None, limit=50):
        if epoch is None:
            epoch = self.num_epochs - 1

        if context == 'test':
            df = self.test_df
            epoch = self.last_test_epoch
        else:
            df = self.report

        df = df[(df.subset == context) & (df.epoch == epoch)].sort_values(by=['loss'], ascending=False).head(50)
        rows = []
        pred = []

        for i in range(limit):
            sample = df.iloc[i]
            try:
                dataset_idx = self.dm.datasets[context].get_row(sample.key, sample.start)
                rows.append(dataset_idx)
            except KeyError as e:
                print(f'Error: {str(e)}')
                continue

            preds = np.fromstring(sample.scores[1:-1], dtype=float, sep=', ')
            preds = torch.from_numpy(preds)
            pred.append(preds)

        pred = torch.stack(pred, dim=0)
        return self.get_sample_plots(indices=rows, pred=pred, context=context, label=None)

    def integrity_check(self) -> bool:
        df = self.report

        # val is deterministic
        for index, row in df[df.subset == 'val'].sample(n=10).iterrows():
            assert len(df[(df.key == row.key) & (df.start == row.start)]) == self.num_epochs

        # test is deterministic
        if self.num_test_runs > 0:
            for index, row in self.test_df.sample(n=10).iterrows():
                assert len(self.test_df[(self.test_df.key == row.key) & (self.test_df.start == row.start)]) == 1
                # no test duplicates
                assert len(self.test_df[(self.test_df.key == row.key) & (self.test_df.start == row.start) & (
                            self.test_df.epoch == row.epoch)]) == 1

        # train is not deterministic
        occs = []
        for index, row in df[df.subset == 'train'].sample(n=10).iterrows():
            occ = len(df[(df.key == row.key) & (df.start == row.start)])
            assert occ <= self.num_epochs
            occs.append(occ)

        # occurances should be different
        occs = [occ == occs[0] for occ in occs]
        assert False in occs

        return True

    def _label_occurances(self, split: str, epoch: int, label: str):
        df = self.report
        return len(df[(df.subset == split) & (df.epoch == epoch) & (df.labels.str.contains(label, na=False))])

    def _background_occurances(self, split: str, epoch: int):
        df = self.report
        return len(df[(df.subset == split) & (df.epoch == epoch) & (df.labels.str.contains('nan', na=True))])

    def _get_y(self, split: str, epoch: int):
        if split == 'test':
            df = self.test_df
        else:
            df = self.report
            df = df[(df.subset == split) & (df.epoch == epoch)]
        y = df.y.tolist()
        y = [np.fromstring(score[1:-1], sep=', ') for score in y]
        y = torch.Tensor(y)
        return y

    def _get_scores(self, split: str, epoch: int):
        if split == 'test':
            df = self.test_df
        else:
            df = self.report
            df = df[(df.subset == split) & (df.epoch == epoch)]
        out = df.scores.tolist()
        out = [np.fromstring(score[1:-1], sep=', ') for score in out]
        out = torch.Tensor(out)
        return out

    def train_samples_boxplot(self, save=True, upload=True):
        fig, ax = plt.subplots(dpi=120)

        plt.xticks(rotation=90)

        ax.set_title(f'train samples per epoch')

        occs = [[self._label_occurances('train', epoch, label) for epoch in range(self.num_epochs)] for label in
                self.dm.classes]
        occs = occs + [[self._background_occurances('train', epoch) for epoch in range(self.num_epochs)]]

        ax.hlines(self.dm.limit_per_class['train'], -0.5, 33.5, color='grey')
        ax.boxplot(occs, vert=True, patch_artist=True, labels=self.dm.classes + ['background'])

        plt.tight_layout()
        plt.close()

        self._handle(fig, 'train', f'samples', save, upload)

        return fig

    def _init_train_scalars(self, epoch: int):
        if self.train_scalars[epoch] is None:
            scalars = MultiLabelStatScores(self.dm.num_classes, threshold=0.5)
            scalars(self._get_scores('train', epoch), self._get_y('train', epoch))
            self.train_scalars[epoch] = scalars

            scalars = MultiLabelStatScores(self.dm.num_classes, threshold=0.5)
            scalars(self._get_scores('val', epoch), self._get_y('val', epoch))
            self.val_scalars[epoch] = scalars

    def _init_test_scalars(self, threshold: int):
        assert -1 < threshold < 101
        if self.test_scalars[threshold] is None:
            scalars = MultiLabelStatScores(self.dm.num_classes, threshold=threshold / 100.0)
            scalars(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
            self.test_scalars[threshold] = scalars

    def _init_train_curve(self):
        if self.train_curve is None:
            curve = MultiLabelStatCurves(self.dm.num_classes)
            curve(self._get_scores('train', self.num_epochs - 1), self._get_y('train', self.num_epochs - 1))
            self.train_curve = curve

    def _init_val_curve(self):
        if self.val_curve is None:
            curve = MultiLabelStatCurves(self.dm.num_classes)
            curve(self._get_scores('val', self.num_epochs - 1), self._get_y('val', self.num_epochs - 1))
            self.val_curve = curve

    def _init_test_curve(self):
        if self.test_curve is None:
            curve = MultiLabelStatCurves(self.dm.num_classes)
            curve(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
            self.test_curve = curve

    def get_metric_by_epoch(self, metric: str, reduction: str, save=True, upload=False):
        assert reduction in ['micro', 'macro', 'weighted']

        fig, ax = plt.subplots(dpi=120)
        ax.set_title(f'{reduction} {metric}')

        train = []
        val = []
        for epoch in range(self.num_epochs):
            self._init_train_scalars(epoch)
            train.append(getattr(self.train_scalars[epoch], metric)(class_reduction=reduction))
            val.append(getattr(self.val_scalars[epoch], metric)(class_reduction=reduction))

        ax.plot(train, label='train data')
        ax.plot(val, label='val data')
        ax.legend()

        plt.tight_layout()
        plt.close()
        if upload:
            self.logger.log_metrics({
                f'train_{metric}_{reduction}': train[-1],
                f'val_{metric}_{reduction}': val[-1]
            })

        self._handle(fig, 'train', f'{reduction} {metric} while training', save, upload)
        return fig

    def get_curve(self, split: str, metric: str, reductions: [str], classes: [str], save=True, upload=False):
        curve: Optional[MultiLabelStatCurves] = None

        if split == 'train':
            self._init_train_curve()
            curve = self.train_curve
        elif split == 'val':
            self._init_val_curve()
            curve = self.val_curve
        elif split == 'test':
            self._init_test_curve()
            curve = self.test_curve

        curves = getattr(curve, metric)(['micro', 'macro'], [i for i in range(len(self.dm.classes))])
        fig, ax = plt.subplots(dpi=120)
        ax.set_title(f'{metric}')
        ax.set_xlabel('false-positive rate (fpr)')
        ax.set_ylabel('true-positive rate (tpr)')

        ax.plot([0, 1], [0, 1], linestyle='--')

        labels = reductions + self.dm.classes
        keys = reductions + [i for i in range(self.dm.num_classes)]

        metrics = {}
        for idx, key in enumerate(keys):
            fpr, tpr, thresholds, peak = curves[key]
            label = labels[idx]
            threshold = thresholds[peak]
            metrics[f'{split}_{metric}_threshold_{label}'] = threshold
            if label in reductions or label in classes:
                color = next(ax._get_lines.prop_cycler)['color']
                linestyle = '-' if label in classes else ':'
                label = (f'{label}: θ=%0.2f' % threshold) if label in classes else label
                ax.plot(fpr, tpr, color=color, label=label, linestyle=linestyle)
                ax.plot(fpr[peak], tpr[peak], color=color, marker='o')

        ax.legend()

        plt.tight_layout()
        plt.close()

        print(metrics)

        if upload == True:
            self.logger.log_metrics(metrics)
        self._handle(fig, 'train', f'samples', save, upload)
        return fig

    def get_scalars(self, save=True, upload=False):

        splits = ['train', 'val', 'test']
        if self.num_test_runs == 0:
            splits.pop()

        scalars: Optional[MultiLabelStatScores] = None
        curve: Optional[MultiLabelStatCurves] = None
        fig, ax = plt.subplots(dpi=120)

        labels = []
        values = dict()

        for idx, split in enumerate(splits):
            if split == 'train':
                self._init_train_curve()
                self._init_train_scalars(self.num_epochs - 1)
                curve = self.train_curve
                scalars = self.train_scalars[self.num_epochs - 1]
            elif split == 'val':
                self._init_val_curve()
                self._init_train_scalars(self.num_epochs - 1)
                curve = self.val_curve
                scalars = self.val_scalars[self.num_epochs - 1]
            elif split == 'test':
                self._init_test_curve()
                self._init_test_scalars(50)
                curve = self.test_curve
                scalars = self.test_scalars[50]
            metrics = {
                f'{split}_balanced_accuracy_micro': scalars.balanced_accuracy('micro'),
                f'{split}_balanced_accuracy_macro': scalars.balanced_accuracy('macro'),
                f'{split}_balanced_accuracy_weighted': scalars.balanced_accuracy('weighted'),
                f'{split}_accuracy_micro': scalars.accuracy('micro'),
                f'{split}_accuracy_macro': scalars.accuracy('macro'),
                f'{split}_accuracy_weighted': scalars.accuracy('weighted'),
                f'{split}_f1_micro': scalars.f1('micro'),
                f'{split}_f1_macro': scalars.f1('macro'),
                f'{split}_f1_weighted': scalars.f1('weighted'),
                f'{split}_precision_micro': scalars.precision('micro'),
                f'{split}_precision_macro': scalars.precision('macro'),
                f'{split}_precision_weighted': scalars.precision('weighted'),
                f'{split}_recall_micro': scalars.recall('micro'),
                f'{split}_recall_macro': scalars.recall('macro'),
                f'{split}_recall_weighted': scalars.recall('weighted'),
                f'{split}_auroc_micro': curve.auroc('micro'),
                f'{split}_auroc_macro': curve.auroc('macro'),
                f'{split}_auroc_weighted': curve.auroc('macro'),
            }

            labels = [key[len(split) + 1:] for key, value in metrics.items()]
            values[split] = [float(value) for key, value in metrics.items()]

            if upload:
                self.logger.log_metrics(metrics)

        df = pd.DataFrame({'train': values['train'],
                           'val': values['val'],
                           'test': values['test']
                           }, index=labels)
        df.plot.barh(stacked=False, title='scalar metrics by subset', ax=ax)

        plt.tight_layout()
        plt.close()
        self._handle(fig, 'train', f'scalars per subset', save, upload)
        return fig

    def get_scalar_by_class(self, split: str, metric: str, save=True, upload=False):
        scalars: Optional[MultiLabelStatScores] = None
        curve: Optional[MultiLabelStatCurves] = None
        fig, ax = plt.subplots(dpi=120)
        title = f'{metric} by class'

        if split == 'train':
            self._init_train_curve()
            self._init_train_scalars(self.num_epochs - 1)
            curve = self.train_curve
            scalars = self.train_scalars[self.num_epochs - 1]
        elif split == 'val':
            self._init_val_curve()
            self._init_train_scalars(self.num_epochs - 1)
            curve = self.val_curve
            scalars = self.val_scalars[self.num_epochs - 1]
        elif split == 'test':
            self._init_test_curve()
            self._init_test_scalars(50)
            curve = self.test_curve
            scalars = self.test_scalars[50]

        metric_fn = getattr(scalars, metric) if metric != 'auroc' else curve.auroc
        scalars = metric_fn('none')
        order = torch.argsort(torch.Tensor(scalars), descending=False).tolist()
        metrics = dict()

        for idx in order:
            cls = self.dm.classes[idx]
            metrics[f'{split}_{metric}_{cls}'] = scalars[idx]

        values = [float(value) for key, value in metrics.items()]
        classes = [self.dm.classes[idx] for idx in order]
        df = pd.DataFrame({split: values}, index=classes)
        df.plot.barh(stacked=False, title=title, ax=ax, legend=False)

        if upload:
            self.logger.log_metrics(metrics)

        plt.tight_layout()
        plt.close()
        self._handle(fig, 'train', title, save, upload)
        return fig

    def cluster_classes_by_metrics(self, split: str, save=True, upload=False):
        pass

    def get_threshold_by_metric(self, split: str, metric: str, reduction: str, save=True, upload=False):
        fig, ax = plt.subplots(dpi=120)
        ax.set_title(f'threshold by {reduction} {metric}')
        ax.set_xlabel('threshold')
        ax.set_ylabel(f'score')

        for i in range(100):
            self._init_test_scalars(i)
        # todo: support classes like in get_curve
        x = torch.linspace(0.01, 1.0, 100)
        metrics = [getattr(scalar, metric)(reduction) for scalar in self.test_scalars]
        color = next(ax._get_lines.prop_cycler)['color']
        peak = torch.argmax(torch.Tensor(metrics)).item()
        ax.plot(x, metrics, color=color)
        ax.plot(x[peak], metrics[peak], color=color, marker='o')

        if upload:
            self.logger.log_metrics({f'threshold_by_{metric}_{reduction}': x[peak]})

        plt.tight_layout()
        plt.close()
        self._handle(fig, split, f'threshold by {reduction} {metric}', save, upload)
        return fig

    def get_metrics_by_consensus(self, save=True, upload=False):
        fig, ax = plt.subplots(dpi=120)

        stored_consensus = self.consensus
        metrics = dict()

        self.consensus = 'max'
        curve = MultiLabelStatCurves(self.dm.num_classes)
        curve(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
        scalars = MultiLabelStatScores(self.dm.num_classes, threshold=50 / 100.0)
        scalars(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
        metrics['max'] = {
            f'test_balanced_accuracy_macro_{self.consensus}_consensus': float(scalars.balanced_accuracy('macro')),
            f'test_accuracy_macro_{self.consensus}_consensus': float(scalars.accuracy('macro')),
            f'test_f1_macro_{self.consensus}_consensus': float(scalars.f1('macro')),
            f'test_precision_macro_{self.consensus}_consensus': float(scalars.precision('macro')),
            f'test_recall_macro_{self.consensus}_consensus': float(scalars.recall('macro')),
            f'test_auroc_macro_{self.consensus}_consensus': curve.auroc('macro'),
        }

        self.consensus = 'avg'
        curve = MultiLabelStatCurves(self.dm.num_classes)
        curve(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
        scalars = MultiLabelStatScores(self.dm.num_classes, threshold=50 / 100.0)
        scalars(self._get_scores('test', self.last_test_epoch), self._get_y('test', self.last_test_epoch))
        metrics['avg'] = {
            f'test_balanced_accuracy_macro_{self.consensus}_consensus': float(scalars.balanced_accuracy('macro')),
            f'test_accuracy_macro_{self.consensus}_consensus': float(scalars.accuracy('macro')),
            f'test_f1_macro_{self.consensus}_consensus': float(scalars.f1('macro')),
            f'test_precision_macro_{self.consensus}_consensus': float(scalars.precision('macro')),
            f'test_recall_macro_{self.consensus}_consensus': float(scalars.recall('macro')),
            f'test_auroc_macro_{self.consensus}_consensus': curve.auroc('macro'),
        }

        self.consensus = stored_consensus

        max = [v for k, v in metrics['max'].items()]
        avg = [v for k, v in metrics['avg'].items()]

        index = ['balanced accuracy', 'accuracy', 'f1',
                 'precision', 'recall', 'auroc']
        df = pd.DataFrame({'max': max,
                           'avg': avg}, index=index)
        df.plot.barh(stacked=False, title='consensus functions', ax=ax, ylabel='macro')

        if upload:
            self.logger.log_metrics(metrics['max'])
            self.logger.log_metrics(metrics['avg'])

        self._handle(fig, 'test', f'metrics by consensus', save, upload)

    @property
    def num_epochs(self):
        df = self.report
        return df[df.subset == 'train'].epoch.nunique()

    @property
    def num_test_runs(self):
        df = self.report
        df = df[df.subset == 'test']
        duplicates = df.duplicated(subset=['subset', 'key', 'start']).sum() > 0
        if duplicates == True:
            return 2
        return self.report[self.report.subset == 'test'].epoch.nunique()

    @property
    def last_test_epoch(self):
        return self.report[self.report.subset == 'test'].epoch.max()

    @property
    def test_df(self):
        df = self.report
        df = df[df.subset == 'test']
        if self.consensus == 'max':
            return df.drop_duplicates(subset=['subset', 'key', 'start'], keep='first')
        elif self.consensus == 'avg':
            return df.drop_duplicates(subset=['subset', 'key', 'start'], keep='last')
